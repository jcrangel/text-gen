{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"word_gen_emb_pretrained.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"gnH8yI3795gA","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"ZeDSjThJBP1O","colab_type":"code","outputId":"2c046565-63f0-4271-9334-7beaed1af7e0","executionInfo":{"status":"ok","timestamp":1573757505475,"user_tz":360,"elapsed":14952,"user":{"displayName":"yuyo hakusho","photoUrl":"","userId":"15919232548116528887"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vxPDMDowBbxh","colab_type":"code","outputId":"6b6c772c-0802-4915-d725-90b83d316541","executionInfo":{"status":"ok","timestamp":1573757505479,"user_tz":360,"elapsed":1037,"user":{"displayName":"yuyo hakusho","photoUrl":"","userId":"15919232548116528887"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd gdrive/My\\ Drive/Colab\\ Notebooks/text-gen"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Colab Notebooks/text-gen\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OKq1SZcuBy0s","colab_type":"code","colab":{}},"source":["# !git remote add origin https://jcrangel:pass@github.com/jcrangel/text-gen.git"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NlLxJzPOoiOF","colab_type":"code","colab":{}},"source":["# !git remote remove origin"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CD42Psv0DV0o","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"a382460f-953a-49f6-cff4-a40c1710c9cc","executionInfo":{"status":"ok","timestamp":1573770857317,"user_tz":360,"elapsed":4336,"user":{"displayName":"yuyo hakusho","photoUrl":"","userId":"15919232548116528887"}}},"source":["# !git push origin master"],"execution_count":161,"outputs":[{"output_type":"stream","text":["Counting objects: 4, done.\n","Delta compression using up to 4 threads.\n","Compressing objects:  25% (1/4)   \rCompressing objects:  50% (2/4)   \rCompressing objects:  75% (3/4)   \rCompressing objects: 100% (4/4)   \rCompressing objects: 100% (4/4), done.\n","Writing objects:  25% (1/4)   \rWriting objects:  50% (2/4)   \rWriting objects:  75% (3/4)   \rWriting objects: 100% (4/4)   \rWriting objects: 100% (4/4), 5.53 KiB | 1.38 MiB/s, done.\n","Total 4 (delta 3), reused 0 (delta 0)\n","remote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\n","To https://github.com/jcrangel/text-gen.git\n","   914b892..7d0b653  master -> master\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"My4_4gv2Hvyc","colab_type":"text"},"source":["## Process text"]},{"cell_type":"code","metadata":{"id":"hMtHzsmsVxzV","colab_type":"code","colab":{}},"source":["import string\n","\n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n","\n","# turn a doc into clean tokens\n","def clean_doc(doc):\n","\t# replace '--' with a space ' '\n","\tdoc = doc.replace('--', ' ')\n","\t# split into tokens by white space\n","\ttokens = doc.split()\n","\t# remove punctuation from each token\n","\ttable = str.maketrans('', '', string.punctuation)\n","\ttokens = [w.translate(table) for w in tokens]\n","\t# remove remaining tokens that are not alphabetic\n","\ttokens = [word for word in tokens if word.isalpha()]\n","\t# make lower case\n","\ttokens = [word.lower() for word in tokens]\n","\treturn tokens\n","\n","# save tokens to file, one dialog per line\n","def save_doc(lines, filename):\n","\tdata = '\\n'.join(lines)\n","\tfile = open(filename, 'w')\n","\tfile.write(data)\n","\tfile.close()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bbtBVnb0Biww","colab_type":"code","outputId":"ad123103-9862-491b-c4d3-28d5fd521a87","executionInfo":{"status":"ok","timestamp":1573757534608,"user_tz":360,"elapsed":1578,"user":{"displayName":"yuyo hakusho","photoUrl":"","userId":"15919232548116528887"}},"colab":{"base_uri":"https://localhost:8080/","height":224}},"source":["\n","# load document\n","in_filename = 'fables_aesop.txt'\n","doc = load_doc(in_filename)\n","print(doc[:200])\n","\n","# clean document\n","tokens = clean_doc(doc)\n","print(tokens[:200])\n","print('Total Tokens: %d' % len(tokens))\n","print('Unique Tokens: %d' % len(set(tokens)))\n","\n","# organize into sequences of tokens\n","length = 50 + 1\n","sequences = list()\n","for i in range(length, len(tokens)):\n","\t# select sequence of tokens\n","\tseq = tokens[i-length:i]\n","\t# convert into a line\n","\tline = ' '.join(seq)\n","\t# store\n","\tsequences.append(line)\n","print('Total Sequences: %d' % len(sequences))\n","\n","# save sequences to file\n","out_filename = 'fables_aesop_sequences.txt'\n","save_doc(sequences, out_filename)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["THE WOLF AND THE LAMB\n","\n","\n","ONE day a Wolf and a Lamb happened to come at the same time to drink\n","from a brook that ran down the side of the mountain.\n","\n","The Wolf wished very much to eat the Lamb, but meetin\n","['the', 'wolf', 'and', 'the', 'lamb', 'one', 'day', 'a', 'wolf', 'and', 'a', 'lamb', 'happened', 'to', 'come', 'at', 'the', 'same', 'time', 'to', 'drink', 'from', 'a', 'brook', 'that', 'ran', 'down', 'the', 'side', 'of', 'the', 'mountain', 'the', 'wolf', 'wished', 'very', 'much', 'to', 'eat', 'the', 'lamb', 'but', 'meeting', 'her', 'as', 'he', 'did', 'face', 'to', 'face', 'he', 'thought', 'he', 'must', 'find', 'some', 'excuse', 'for', 'doing', 'so', 'so', 'he', 'began', 'by', 'trying', 'to', 'pick', 'a', 'quarrel', 'and', 'said', 'angrily', 'dare', 'you', 'come', 'to', 'my', 'brook', 'and', 'muddy', 'the', 'water', 'so', 'that', 'i', 'cannot', 'drink', 'it', 'what', 'do', 'you', 'the', 'lamb', 'very', 'much', 'alarmed', 'said', 'gently', 'do', 'not', 'see', 'how', 'it', 'can', 'be', 'that', 'i', 'have', 'spoiled', 'the', 'water', 'you', 'stand', 'higher', 'up', 'the', 'stream', 'and', 'the', 'water', 'runs', 'from', 'you', 'to', 'me', 'not', 'from', 'me', 'to', 'that', 'as', 'it', 'said', 'the', 'wolf', 'with', 'a', 'snarl', 'are', 'a', 'rascal', 'all', 'the', 'same', 'for', 'i', 'have', 'heard', 'that', 'last', 'year', 'you', 'said', 'bad', 'things', 'of', 'me', 'behind', 'my', 'mr', 'cried', 'the', 'poor', 'lamb', 'could', 'not', 'be', 'for', 'a', 'year', 'ago', 'i', 'was', 'not', 'born', 'i', 'am', 'only', 'six', 'months', 'finding', 'it', 'of', 'no', 'use', 'to', 'argue', 'any', 'more', 'the', 'wolf', 'began', 'to', 'snarl', 'and', 'show', 'his', 'teeth', 'coming', 'closer']\n","Total Tokens: 88800\n","Unique Tokens: 7824\n","Total Sequences: 88749\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Gft2Tt9MHzT5","colab_type":"text"},"source":["## get training data"]},{"cell_type":"markdown","metadata":{"id":"Dng-28iFkKWo","colab_type":"text"},"source":["Read the vector from glove file"]},{"cell_type":"code","metadata":{"id":"khNS6O1_Weo5","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","def read_glove_vecs(glove_file):\n","    with open(glove_file, 'r') as f:\n","        words = set()\n","        word_to_vec_map = {}\n","        for line in f:\n","            line = line.strip().split()\n","            curr_word = line[0]\n","            words.add(curr_word)\n","            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n","        \n","        i = 1\n","        words_to_index = {}\n","        index_to_words = {}\n","        for w in sorted(words):\n","            words_to_index[w] = i\n","            index_to_words[i] = w\n","            i = i + 1\n","\n","    return words_to_index, index_to_words, word_to_vec_map"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mbLhpb1OkJoF","colab_type":"code","colab":{}},"source":["word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b2ly46zKj84g","colab_type":"code","colab":{}},"source":["# GRADED FUNCTION: sentences_to_indices\n","\n","def sentences_to_indices(X, word_to_index, index_to_word, word_to_vec_map, max_len):\n","    \"\"\"\n","    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n","    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n","    \n","    Arguments:\n","    X -- array of sentences (strings), of shape (m, 1)\n","    word_to_index -- a dictionary containing the each word mapped to its index\n","    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n","    \n","    Returns:\n","    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n","    \"\"\"\n","    # X = np.array(X)\n","    m = X.shape[0]                                   # number of training examples\n","    vec_len = word_to_vec_map['baseball'].shape[0]\n","    ### START CODE HERE ###\n","    # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)\n","    X_indices = np.zeros( shape = (m, max_len))\n","    \n","    for i in range(m):                               # loop over training examples\n","        \n","        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n","        # sentence_words = list(map(lambda x : x.lower() , X[i].split(\" \")))\n","        sentence_words = [i.lower() for i in X[i].split()]\n","        # Initialize j to 0\n","        j = 0\n","        \n","        # Loop over the words of sentence_words\n","        for w in sentence_words:\n","            # Set the (i,j)th entry of X_indices to the index of the correct word.\n","            try:\n","              X_indices[i, j] = word_to_index[w]\n","            except KeyError:\n","              print(w + \" doesnt have index, new entry created\")\n","              \n","              vocab_size = len(word_to_index) + 1\n","              word_to_index[w] = vocab_size\n","              X_indices[i, j] = vocab_size\n","              word_to_vec_map[w] = np.random.rand(vec_len)\n","              index_to_word[vocab_size] = w \n","              continue\n","            # Increment j to j + 1\n","            j += 1\n","            if j >= max_len:\n","              break\n","    ### END CODE HERE ###\n","    # pdb.set_trace()\n","    return X_indices"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_K3kkn1J0nfj","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"D8mSuOZOkTRt","colab_type":"code","outputId":"56fa2cdb-58ed-4f91-b00d-1edae4d4f8cf","executionInfo":{"status":"ok","timestamp":1573757584337,"user_tz":360,"elapsed":5902,"user":{"displayName":"yuyo hakusho","photoUrl":"","userId":"15919232548116528887"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n","X1_indices = sentences_to_indices(X1,word_to_index, index_to_word, word_to_vec_map, max_len = 5)\n","print(\"X1 =\", X1)\n","print(\"X1_indices =\", X1_indices)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["> <ipython-input-8-0e7c0e29fbb4>(21)sentences_to_indices()\n","-> X_indices = np.zeros( shape = (m, max_len))\n","(Pdb) c\n","X1 = ['funny lol' 'lets play baseball' 'food is ready for you']\n","X1_indices = [[155345. 225122.      0.      0.      0.]\n"," [220930. 286375.  69714.      0.      0.]\n"," [151204. 192973. 302254. 151349. 394475.]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oslLS2XUBssR","colab_type":"code","outputId":"5b02bbc6-e695-4829-e1d8-3c1687495348","executionInfo":{"status":"ok","timestamp":1573757616604,"user_tz":360,"elapsed":1705,"user":{"displayName":"yuyo hakusho","photoUrl":"","userId":"15919232548116528887"}},"colab":{"base_uri":"https://localhost:8080/","height":63}},"source":["from numpy import array\n","from pickle import dump\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import LSTM\n","from tensorflow.keras.layers import Embedding\n","from tensorflow.keras.layers import Bidirectional \n"],"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"ZlR6QZXVkcot","colab_type":"code","colab":{}},"source":["# load\n","in_filename = 'fables_aesop_sequences.txt'\n","doc = load_doc(in_filename)\n","lines = doc.split('\\n')\n","lines = np.array(lines)\n","length = 50 + 1\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_j7VoWn70v3W","colab_type":"code","outputId":"d4bfd2b2-9430-4242-e67d-d157e393ebad","executionInfo":{"status":"ok","timestamp":1573757623043,"user_tz":360,"elapsed":403,"user":{"displayName":"yuyo hakusho","photoUrl":"","userId":"15919232548116528887"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["print(lines[:5])"],"execution_count":13,"outputs":[{"output_type":"stream","text":["['the wolf and the lamb one day a wolf and a lamb happened to come at the same time to drink from a brook that ran down the side of the mountain the wolf wished very much to eat the lamb but meeting her as he did face to face he'\n"," 'wolf and the lamb one day a wolf and a lamb happened to come at the same time to drink from a brook that ran down the side of the mountain the wolf wished very much to eat the lamb but meeting her as he did face to face he thought'\n"," 'and the lamb one day a wolf and a lamb happened to come at the same time to drink from a brook that ran down the side of the mountain the wolf wished very much to eat the lamb but meeting her as he did face to face he thought he'\n"," 'the lamb one day a wolf and a lamb happened to come at the same time to drink from a brook that ran down the side of the mountain the wolf wished very much to eat the lamb but meeting her as he did face to face he thought he must'\n"," 'lamb one day a wolf and a lamb happened to come at the same time to drink from a brook that ran down the side of the mountain the wolf wished very much to eat the lamb but meeting her as he did face to face he thought he must find']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U9LBplvPHvtL","colab_type":"code","outputId":"11c32eff-4656-4e54-e82d-de11d2cf5bcf","executionInfo":{"status":"ok","timestamp":1573757625400,"user_tz":360,"elapsed":357,"user":{"displayName":"yuyo hakusho","photoUrl":"","userId":"15919232548116528887"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(lines[5])"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["230"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"1R0N9cTA7uUR","colab_type":"code","outputId":"56e89d68-8f24-4454-bab3-0fcc61f1821c","executionInfo":{"status":"ok","timestamp":1573757665463,"user_tz":360,"elapsed":6277,"user":{"displayName":"yuyo hakusho","photoUrl":"","userId":"15919232548116528887"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# integer encode sequences of words\n","#okenizer = Tokenizer()\n","#tokenizer.fit_on_texts(lines)\n","\n","#array([   15,     1,   306, 10270,    28,     7,   363,   225,  4604,....])\n","sequences = sentences_to_indices(lines,word_to_index, index_to_word, word_to_vec_map, max_len = length)\n","# vocabulary size\n","vocab_size = len(word_to_index) + 1  \n","X, y = sequences[:,:-1], sequences[:,-1]\n","#y = to_categorical(y, num_classes=vocab_size) eats all memory use sparse_categorical_crossentropy\n","seq_length = X.shape[1]"],"execution_count":15,"outputs":[{"output_type":"stream","text":["> <ipython-input-10-713f64c774b3>(22)sentences_to_indices()\n","-> X_indices = np.zeros( shape = (m, max_len))\n","(Pdb) c\n","famishing doesnt have index, new entry created\n","storkking doesnt have index, new entry created\n","grayear doesnt have index, new entry created\n","whitewhisker doesnt have index, new entry created\n","gayly doesnt have index, new entry created\n","unsuspicious doesnt have index, new entry created\n","capered doesnt have index, new entry created\n","selfimportance doesnt have index, new entry created\n","dainties doesnt have index, new entry created\n","manyfeathered doesnt have index, new entry created\n","oneeyed doesnt have index, new entry created\n","repine doesnt have index, new entry created\n","filberts doesnt have index, new entry created\n","illmannered doesnt have index, new entry created\n","cowhides doesnt have index, new entry created\n","fairtime doesnt have index, new entry created\n","wellproved doesnt have index, new entry created\n","dinnerless doesnt have index, new entry created\n","miry doesnt have index, new entry created\n","selfinterest doesnt have index, new entry created\n","evildisposed doesnt have index, new entry created\n","faultfinding doesnt have index, new entry created\n","rushlight doesnt have index, new entry created\n","relighted doesnt have index, new entry created\n","wisping doesnt have index, new entry created\n","starveling doesnt have index, new entry created\n","puffedout doesnt have index, new entry created\n","scapegrace doesnt have index, new entry created\n","importunity doesnt have index, new entry created\n","rodiland doesnt have index, new entry created\n","rateater doesnt have index, new entry created\n","unkilled doesnt have index, new entry created\n","moujiks doesnt have index, new entry created\n","æsop doesnt have index, new entry created\n","bethought doesnt have index, new entry created\n","affrighted doesnt have index, new entry created\n","goodnatured doesnt have index, new entry created\n","fourfooted doesnt have index, new entry created\n","mouselike doesnt have index, new entry created\n","illmatched doesnt have index, new entry created\n","dotingly doesnt have index, new entry created\n","surprized doesnt have index, new entry created\n","overanxiety doesnt have index, new entry created\n","bantling doesnt have index, new entry created\n","wellgrown doesnt have index, new entry created\n","shews doesnt have index, new entry created\n","schoolfellows doesnt have index, new entry created\n","meanspirited doesnt have index, new entry created\n","controul doesnt have index, new entry created\n","tutorage doesnt have index, new entry created\n","stiled doesnt have index, new entry created\n","behoves doesnt have index, new entry created\n","jocosely doesnt have index, new entry created\n","intreats doesnt have index, new entry created\n","stopt doesnt have index, new entry created\n","recal doesnt have index, new entry created\n","findeth doesnt have index, new entry created\n","gaminghouses doesnt have index, new entry created\n","sharping doesnt have index, new entry created\n","mistakingly doesnt have index, new entry created\n","irrecoverably doesnt have index, new entry created\n","unwearied doesnt have index, new entry created\n","wellorganized doesnt have index, new entry created\n","œconomy doesnt have index, new entry created\n","fragrancy doesnt have index, new entry created\n","cloaths doesnt have index, new entry created\n","shewed doesnt have index, new entry created\n","illjudged doesnt have index, new entry created\n","shewing doesnt have index, new entry created\n","derogates doesnt have index, new entry created\n","sharpers doesnt have index, new entry created\n","befal doesnt have index, new entry created\n","sheepbiter doesnt have index, new entry created\n","expostulate doesnt have index, new entry created\n","ungratitude doesnt have index, new entry created\n","infelicity doesnt have index, new entry created\n","needeth doesnt have index, new entry created\n","doubletongued doesnt have index, new entry created\n","welldisposed doesnt have index, new entry created\n","gobetweens doesnt have index, new entry created\n","clapt doesnt have index, new entry created\n","dunghill doesnt have index, new entry created\n","allurements doesnt have index, new entry created\n","dropt doesnt have index, new entry created\n","designedly doesnt have index, new entry created\n","selfapprobation doesnt have index, new entry created\n","vizor doesnt have index, new entry created\n","undrest doesnt have index, new entry created\n","embued doesnt have index, new entry created\n","unenvied doesnt have index, new entry created\n","boaster doesnt have index, new entry created\n","expence doesnt have index, new entry created\n","asseverations doesnt have index, new entry created\n","selfconceited doesnt have index, new entry created\n","betimes doesnt have index, new entry created\n","illwill doesnt have index, new entry created\n","fiendlike doesnt have index, new entry created\n","irreconcileable doesnt have index, new entry created\n","unfrequently doesnt have index, new entry created\n","visitants doesnt have index, new entry created\n","officiousness doesnt have index, new entry created\n","pismires doesnt have index, new entry created\n","illnatured doesnt have index, new entry created\n","burthens doesnt have index, new entry created\n","shewn doesnt have index, new entry created\n","aftertimes doesnt have index, new entry created\n","asperse doesnt have index, new entry created\n","periwig doesnt have index, new entry created\n","captious doesnt have index, new entry created\n","disquietude doesnt have index, new entry created\n","selfinterested doesnt have index, new entry created\n","bespatter doesnt have index, new entry created\n","sulkily doesnt have index, new entry created\n","slightingly doesnt have index, new entry created\n","acquirements doesnt have index, new entry created\n","illfortune doesnt have index, new entry created\n","overselfish doesnt have index, new entry created\n","narrowminded doesnt have index, new entry created\n","engross doesnt have index, new entry created\n","apprized doesnt have index, new entry created\n","undeceives doesnt have index, new entry created\n","twelvemonth doesnt have index, new entry created\n","roguery doesnt have index, new entry created\n","illgoverned doesnt have index, new entry created\n","expiates doesnt have index, new entry created\n","burialplace doesnt have index, new entry created\n","insensibly doesnt have index, new entry created\n","stupifies doesnt have index, new entry created\n","oftener doesnt have index, new entry created\n","espied doesnt have index, new entry created\n","unprovided doesnt have index, new entry created\n","tinctured doesnt have index, new entry created\n","forebore doesnt have index, new entry created\n","unblest doesnt have index, new entry created\n","intreated doesnt have index, new entry created\n","farrowed doesnt have index, new entry created\n","obtrudes doesnt have index, new entry created\n","panicstruck doesnt have index, new entry created\n","pisistratus doesnt have index, new entry created\n","bunghole doesnt have index, new entry created\n","phædrus doesnt have index, new entry created\n","burthensome doesnt have index, new entry created\n","importunities doesnt have index, new entry created\n","repinings doesnt have index, new entry created\n","rufflings doesnt have index, new entry created\n","wouldst doesnt have index, new entry created\n","mischiefs doesnt have index, new entry created\n","obtrude doesnt have index, new entry created\n","goodfornothing doesnt have index, new entry created\n","distempers doesnt have index, new entry created\n","lanthorn doesnt have index, new entry created\n","phiz doesnt have index, new entry created\n","oculists doesnt have index, new entry created\n","dogmatical doesnt have index, new entry created\n","perverseness doesnt have index, new entry created\n","bloodo doesnt have index, new entry created\n","longeared doesnt have index, new entry created\n","fourlegged doesnt have index, new entry created\n","halfstarved doesnt have index, new entry created\n","doubledealing doesnt have index, new entry created\n","coxcombs doesnt have index, new entry created\n","drawingroom doesnt have index, new entry created\n","defiances doesnt have index, new entry created\n","dastard doesnt have index, new entry created\n","littleness doesnt have index, new entry created\n","weakminded doesnt have index, new entry created\n","misenus doesnt have index, new entry created\n","officiously doesnt have index, new entry created\n","complaisance doesnt have index, new entry created\n","broils doesnt have index, new entry created\n","wellmeaning doesnt have index, new entry created\n","undiscerning doesnt have index, new entry created\n","wornout doesnt have index, new entry created\n","injuriously doesnt have index, new entry created\n","lyingin doesnt have index, new entry created\n","befool doesnt have index, new entry created\n","pettish doesnt have index, new entry created\n","sirrah doesnt have index, new entry created\n","criminate doesnt have index, new entry created\n","fellowsubjects doesnt have index, new entry created\n","selfconceit doesnt have index, new entry created\n","inconsiderately doesnt have index, new entry created\n","revilings doesnt have index, new entry created\n","familiarities doesnt have index, new entry created\n","agoing doesnt have index, new entry created\n","thieftaker doesnt have index, new entry created\n","raillery doesnt have index, new entry created\n","fullyprepared doesnt have index, new entry created\n","supplicate doesnt have index, new entry created\n","tyrannise doesnt have index, new entry created\n","slabbered doesnt have index, new entry created\n","ingenuously doesnt have index, new entry created\n","hissings doesnt have index, new entry created\n","counterplot doesnt have index, new entry created\n","blackamoor doesnt have index, new entry created\n","illconcerted doesnt have index, new entry created\n","unheeding doesnt have index, new entry created\n","woful doesnt have index, new entry created\n","prithee doesnt have index, new entry created\n","enterprizes doesnt have index, new entry created\n","selfsufficient doesnt have index, new entry created\n","surfeited doesnt have index, new entry created\n","screamings doesnt have index, new entry created\n","unmeaning doesnt have index, new entry created\n","inveigling doesnt have index, new entry created\n","allwise doesnt have index, new entry created\n","misapply doesnt have index, new entry created\n","oxstall doesnt have index, new entry created\n","mastereye doesnt have index, new entry created\n","ringdove doesnt have index, new entry created\n","ashooting doesnt have index, new entry created\n","desponding doesnt have index, new entry created\n","miserableness doesnt have index, new entry created\n","scurvily doesnt have index, new entry created\n","suppliant doesnt have index, new entry created\n","fellowcreatures doesnt have index, new entry created\n","goodnature doesnt have index, new entry created\n","lenity doesnt have index, new entry created\n","brainracking doesnt have index, new entry created\n","unpitied doesnt have index, new entry created\n","impieties doesnt have index, new entry created\n","deeplaid doesnt have index, new entry created\n","froward doesnt have index, new entry created\n","supperless doesnt have index, new entry created\n","apologue doesnt have index, new entry created\n","calumniators doesnt have index, new entry created\n","sots doesnt have index, new entry created\n","precedency doesnt have index, new entry created\n","partaker doesnt have index, new entry created\n","beggarly doesnt have index, new entry created\n","wellstored doesnt have index, new entry created\n","illgotten doesnt have index, new entry created\n","insatiate doesnt have index, new entry created\n","menenius doesnt have index, new entry created\n","illluck doesnt have index, new entry created\n","dovehouse doesnt have index, new entry created\n","chuse doesnt have index, new entry created\n","scrupled doesnt have index, new entry created\n","wellbred doesnt have index, new entry created\n","shamefacedness doesnt have index, new entry created\n","wellfed doesnt have index, new entry created\n","changeableness doesnt have index, new entry created\n","downfal doesnt have index, new entry created\n","unfeelingly doesnt have index, new entry created\n","purseproud doesnt have index, new entry created\n","ingrafted doesnt have index, new entry created\n","enfeebles doesnt have index, new entry created\n","affronting doesnt have index, new entry created\n","illlanguage doesnt have index, new entry created\n","placeman doesnt have index, new entry created\n","playfellow doesnt have index, new entry created\n","parings doesnt have index, new entry created\n","diningroom doesnt have index, new entry created\n","townlife doesnt have index, new entry created\n","pellmell doesnt have index, new entry created\n","selfpreservation doesnt have index, new entry created\n","soever doesnt have index, new entry created\n","loobies doesnt have index, new entry created\n","humouring doesnt have index, new entry created\n","mischance doesnt have index, new entry created\n","deservest doesnt have index, new entry created\n","overflowings doesnt have index, new entry created\n","inclemency doesnt have index, new entry created\n","dissembler doesnt have index, new entry created\n","jackalls doesnt have index, new entry created\n","warsaddle doesnt have index, new entry created\n","inadvertency doesnt have index, new entry created\n","hidingplace doesnt have index, new entry created\n","knavish doesnt have index, new entry created\n","equivocates doesnt have index, new entry created\n","inviolably doesnt have index, new entry created\n","unremitted doesnt have index, new entry created\n","unbend doesnt have index, new entry created\n","slily doesnt have index, new entry created\n","envenomed doesnt have index, new entry created\n","dissentions doesnt have index, new entry created\n","illusage doesnt have index, new entry created\n","ungirded doesnt have index, new entry created\n","packsaddle doesnt have index, new entry created\n","fellowservant doesnt have index, new entry created\n","newsown doesnt have index, new entry created\n","poltroon doesnt have index, new entry created\n","gallanted doesnt have index, new entry created\n","equinamity doesnt have index, new entry created\n","fullbodied doesnt have index, new entry created\n","uneasinesses doesnt have index, new entry created\n","tuftcovered doesnt have index, new entry created\n","peculation doesnt have index, new entry created\n","intrusted doesnt have index, new entry created\n","knawed doesnt have index, new entry created\n","depicture doesnt have index, new entry created\n","prepossess doesnt have index, new entry created\n","viands doesnt have index, new entry created\n","provocatives doesnt have index, new entry created\n","gamesome doesnt have index, new entry created\n","titbit doesnt have index, new entry created\n","likings doesnt have index, new entry created\n","overgrasping doesnt have index, new entry created\n","fomenter doesnt have index, new entry created\n","twoedged doesnt have index, new entry created\n","widespreading doesnt have index, new entry created\n","vollies doesnt have index, new entry created\n","hardheartedness doesnt have index, new entry created\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KIg0S5kn70F4","colab_type":"code","outputId":"a90045e4-ec95-4cbc-9d91-f06061ddadb3","executionInfo":{"status":"ok","timestamp":1573759085856,"user_tz":360,"elapsed":324,"user":{"displayName":"yuyo hakusho","photoUrl":"","userId":"15919232548116528887"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["print(X.shape)\n","print(y.shape)\n","print(len(lines[30].split()))"],"execution_count":40,"outputs":[{"output_type":"stream","text":["(88749, 50)\n","(88749,)\n","51\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lBNStpd5m2gg","colab_type":"code","outputId":"7ba284d1-04f7-4473-b625-27351744d6d8","executionInfo":{"status":"ok","timestamp":1573757679190,"user_tz":360,"elapsed":457,"user":{"displayName":"yuyo hakusho","photoUrl":"","userId":"15919232548116528887"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["word_to_index['mermaid']"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["241527"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"z1P0-UUtCle8","colab_type":"text"},"source":["## Pretrain embedding layer"]},{"cell_type":"markdown","metadata":{"id":"c8AXXgK-oGuN","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"2CQCvNb3CIVN","colab_type":"code","colab":{}},"source":["\n","def pretrained_embedding_layer(word_to_vec_map, word_to_index, trainable = False):\n","    \"\"\"\n","    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n","    \n","    Arguments:\n","    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n","    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n","\n","    Returns:\n","    embedding_layer -- pretrained layer Keras instance\n","    \"\"\"\n","    \n","    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n","    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n","    \n","    ### START CODE HERE ###\n","    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n","    emb_matrix = np.zeros(shape=(vocab_len,emb_dim))\n","    \n","    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n","    for word, index in word_to_index.items():\n","        emb_matrix[index, :] = word_to_vec_map[word]\n","\n","    # Define Keras embedding layer with the correct output/input sizes, make it non-trainable. Use Embedding(...). Make sure to set trainable=False. \n","    embedding_layer = Embedding(vocab_len,emb_dim,trainable=trainable)\n","    \n","    ### END CODE HERE ###\n","\n","    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n","    embedding_layer.build((None,))\n","    \n","    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n","    embedding_layer.set_weights([emb_matrix])\n","    \n","    return embedding_layer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pBHNrUuVX6Jn","colab_type":"code","outputId":"c78f8507-41ad-4af5-cd94-d83ab9302394","executionInfo":{"status":"error","timestamp":1573672554242,"user_tz":360,"elapsed":1582,"user":{"displayName":"yuyo hakusho","photoUrl":"","userId":"15919232548116528887"}},"colab":{"base_uri":"https://localhost:8080/","height":180}},"source":["embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n","print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-5a61855c3e01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_embedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_vec_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weights[0][1][3] =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pretrained_embedding_layer' is not defined"]}]},{"cell_type":"code","metadata":{"id":"PHvueG3nmsPp","colab_type":"code","colab":{}},"source":["from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.optimizers import Adagrad\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.models import load_model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JZNlbHkVYlAK","colab_type":"code","colab":{}},"source":["\n","# define model\n","model = Sequential()\n","# in : (batch, input_lengh) , [12,3,2,3,4,5,12,..., input_lentgh]\n","\n","model.add(pretrained_embedding_layer(word_to_vec_map,word_to_index,trainable = False))\n","model.add(Bidirectional(LSTM(100, return_sequences=True)) )\n","model.add(Bidirectional(LSTM(100)) )\n","model.add(Dense(100, activation='relu'))\n","model.add(Dense(vocab_size, activation='softmax'))\n","print(model.summary())\n","# compile model\n","opt = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n","# opt = SGD(learning_rate=0.001, momentum=0.0)\n","# opt = Adagrad(learning_rate=0.001)\n","model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","\n","filepath=\"weights-improvement.hdf5\"\n","checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')\n","callbacks_list = [checkpoint]\n","# fit model\n","model.fit(X, y, batch_size=128, epochs=100, callbacks=callbacks_list)\n","\n","# save the model to file\n","model.save('model_embpre.h5')\n","# save the tokenizer\n","dump(tokenizer, open('tokenizer.pkl', 'wb'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MkRxd-h48rKn","colab_type":"text"},"source":["Reload and continue training"]},{"cell_type":"code","metadata":{"id":"BmbPOszOrQFR","colab_type":"code","outputId":"d4c992af-2f0d-4541-9581-9d4f6d24c96b","executionInfo":{"status":"error","timestamp":1573678382655,"user_tz":360,"elapsed":74713,"user":{"displayName":"yuyo hakusho","photoUrl":"","userId":"15919232548116528887"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from tensorflow.keras.models import load_model\n","\n","filepath=\"weights-improvement.hdf5\"\n","model = load_model(filepath)\n","checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')\n","callbacks_list = [checkpoint]\n","# fit model\n","model.fit(X, y, batch_size=128, epochs=100, callbacks=callbacks_list)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 88749 samples\n","Epoch 1/100\n","88704/88749 [============================>.] - ETA: 0s - loss: 2.2813 - acc: 0.4530\n","Epoch 00001: acc improved from -inf to 0.45292, saving model to weights-improvement.hdf5\n","88749/88749 [==============================] - 300s 3ms/sample - loss: 2.2816 - acc: 0.4529\n","Epoch 2/100\n","88704/88749 [============================>.] - ETA: 0s - loss: 2.2676 - acc: 0.4569\n","Epoch 00002: acc improved from 0.45292 to 0.45687, saving model to weights-improvement.hdf5\n","88749/88749 [==============================] - 296s 3ms/sample - loss: 2.2679 - acc: 0.4569\n","Epoch 3/100\n","88704/88749 [============================>.] - ETA: 0s - loss: 2.2621 - acc: 0.4548\n","Epoch 00003: acc did not improve from 0.45687\n","88749/88749 [==============================] - 294s 3ms/sample - loss: 2.2623 - acc: 0.4548\n","Epoch 4/100\n","88704/88749 [============================>.] - ETA: 0s - loss: 2.2622 - acc: 0.4570\n","Epoch 00004: acc improved from 0.45687 to 0.45701, saving model to weights-improvement.hdf5\n","88749/88749 [==============================] - 296s 3ms/sample - loss: 2.2624 - acc: 0.4570\n","Epoch 5/100\n","88704/88749 [============================>.] - ETA: 0s - loss: 2.2366 - acc: 0.4619\n","Epoch 00005: acc improved from 0.45701 to 0.46189, saving model to weights-improvement.hdf5\n","88749/88749 [==============================] - 295s 3ms/sample - loss: 2.2369 - acc: 0.4619\n","Epoch 6/100\n","88704/88749 [============================>.] - ETA: 0s - loss: 2.2162 - acc: 0.4667\n","Epoch 00006: acc improved from 0.46189 to 0.46670, saving model to weights-improvement.hdf5\n","88749/88749 [==============================] - 295s 3ms/sample - loss: 2.2162 - acc: 0.4667\n","Epoch 7/100\n","88704/88749 [============================>.] - ETA: 0s - loss: 2.2167 - acc: 0.4670\n","Epoch 00007: acc improved from 0.46670 to 0.46704, saving model to weights-improvement.hdf5\n","88749/88749 [==============================] - 296s 3ms/sample - loss: 2.2166 - acc: 0.4670\n","Epoch 8/100\n","88704/88749 [============================>.] - ETA: 0s - loss: 2.3050 - acc: 0.4471\n","Epoch 00008: acc did not improve from 0.46704\n","88749/88749 [==============================] - 296s 3ms/sample - loss: 2.3047 - acc: 0.4471\n","Epoch 9/100\n","88704/88749 [============================>.] - ETA: 0s - loss: 2.1842 - acc: 0.4719\n","Epoch 00009: acc improved from 0.46704 to 0.47199, saving model to weights-improvement.hdf5\n","88749/88749 [==============================] - 297s 3ms/sample - loss: 2.1840 - acc: 0.4720\n","Epoch 10/100\n","88704/88749 [============================>.] - ETA: 0s - loss: 2.1858 - acc: 0.4714\n","Epoch 00010: acc did not improve from 0.47199\n","88749/88749 [==============================] - 296s 3ms/sample - loss: 2.1858 - acc: 0.4714\n","Epoch 11/100\n","88704/88749 [============================>.] - ETA: 0s - loss: 2.1625 - acc: 0.4759\n","Epoch 00011: acc improved from 0.47199 to 0.47595, saving model to weights-improvement.hdf5\n","88749/88749 [==============================] - 298s 3ms/sample - loss: 2.1625 - acc: 0.4759\n","Epoch 12/100\n","88704/88749 [============================>.] - ETA: 0s - loss: 2.1690 - acc: 0.4755\n","Epoch 00012: acc did not improve from 0.47595\n","88749/88749 [==============================] - 296s 3ms/sample - loss: 2.1688 - acc: 0.4755\n","Epoch 13/100\n","88704/88749 [============================>.] - ETA: 0s - loss: 2.1784 - acc: 0.4726\n","Epoch 00013: acc did not improve from 0.47595\n","88749/88749 [==============================] - 294s 3ms/sample - loss: 2.1786 - acc: 0.4725\n","Epoch 14/100\n","88704/88749 [============================>.] - ETA: 0s - loss: 2.1965 - acc: 0.4667\n","Epoch 00014: acc did not improve from 0.47595\n","88749/88749 [==============================] - 293s 3ms/sample - loss: 2.1964 - acc: 0.4667\n","Epoch 15/100\n","88704/88749 [============================>.] - ETA: 0s - loss: 2.1351 - acc: 0.4804\n","Epoch 00015: acc improved from 0.47595 to 0.48043, saving model to weights-improvement.hdf5\n","88749/88749 [==============================] - 294s 3ms/sample - loss: 2.1348 - acc: 0.4804\n","Epoch 16/100\n","88704/88749 [============================>.] - ETA: 0s - loss: 2.1288 - acc: 0.4821\n","Epoch 00016: acc improved from 0.48043 to 0.48195, saving model to weights-improvement.hdf5\n","88749/88749 [==============================] - 294s 3ms/sample - loss: 2.1292 - acc: 0.4820\n","Epoch 17/100\n","88704/88749 [============================>.] - ETA: 0s - loss: 2.1417 - acc: 0.4800\n","Epoch 00017: acc did not improve from 0.48195\n","88749/88749 [==============================] - 293s 3ms/sample - loss: 2.1418 - acc: 0.4799\n","Epoch 18/100\n","88704/88749 [============================>.] - ETA: 0s - loss: 2.1333 - acc: 0.4802\n","Epoch 00018: acc did not improve from 0.48195\n","88749/88749 [==============================] - 294s 3ms/sample - loss: 2.1334 - acc: 0.4802\n","Epoch 19/100\n","88704/88749 [============================>.] - ETA: 0s - loss: 2.1216 - acc: 0.4830\n","Epoch 00019: acc improved from 0.48195 to 0.48298, saving model to weights-improvement.hdf5\n","88749/88749 [==============================] - 296s 3ms/sample - loss: 2.1218 - acc: 0.4830\n","Epoch 20/100\n","57088/88749 [==================>...........] - ETA: 1:46 - loss: 2.0660 - acc: 0.4958"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-f78c41d690e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"mOMZBQeZ39Lb","colab_type":"text"},"source":["Generate text sequence "]},{"cell_type":"code","metadata":{"id":"LsADxdI04XXt","colab_type":"code","colab":{}},"source":["def sample(preds, temperature=1.0):\n","    # helper function to sample an index from a probability array\n","    # import pdb; pdb.set_trace()\n","    preds = np.asarray(preds[0]).astype('float64')\n","    preds = np.log(preds) / temperature\n","    exp_preds = np.exp(preds)\n","    preds = exp_preds / np.sum(exp_preds)\n","    probas = np.random.multinomial(1, preds, 1)\n","    return np.argmax(probas)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lQJfUfMpcHVN","colab_type":"code","colab":{}},"source":["# generate a sequence from a language model\n","def generate_seq(model, word_to_index, index_to_word, word_to_vec_map, seed_text, max_len,n_words,temperature = 1):\n","  result = list()\n","  in_text = seed_text\n","  # generate a fixed number of words\n","  # import pdb; pdb.set_trace()\n","  for _ in range(n_words):\n","    # encode the text as integer\n","    # encoded = tokenizer.texts_to_sequences([in_text])[0]\n","    # truncate sequences to a fixed length\n","    # encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n","    encoded = sentences_to_indices(np.array([in_text]),word_to_index, index_to_word, word_to_vec_map, max_len = n_words)\n","    # predict probabilities for each word\n","    # import pdb; pdb.set_trace()\n","    if temperature == 0 :\n","      index_class = model.predict_classes(encoded, verbose=0)[0]\n","    else:\n","      pred_probs = model.predict(encoded, verbose=0)\n","      index_class = sample(pred_probs,temperature=0.2)\n","    #\n","    # map predicted word index to word\n","    out_word = ''\n","    # for word, index in word_to_index.items():\n","    #   if index == index_class:\n","    #     out_word = word\n","    #     break\n","    out_word = index_to_word[index_class]\n","    # append to input\n","    in_text += ' ' + out_word\n","\n","    #work with sentences of maximum lenght\n","    if len(in_text.split()) >= max_len:\n","    # remove the first word\n","      in_text = in_text.split(' ',1)[1] \n","    result.append(out_word)\n","  return ' '.join(result)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9e1xMiG_mjxS","colab_type":"code","colab":{}},"source":["# load cleaned text sequences\n","in_filename = 'fables_aesop_sequences.txt'\n","doc = load_doc(in_filename)\n","lines = doc.split('\\n')\n","seq_length = len(lines[0].split()) - 1\n","\n","# load the model\n","model = load_model('weights-improvement.hdf5')\n","\n","\n","# load the tokenizer\n","# tokenizer = load(open('tokenizer.pkl', 'rb'))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x7CxhQKU4TH8","colab_type":"code","outputId":"67eab6a9-c2e4-4427-be39-102df84b3dca","executionInfo":{"status":"ok","timestamp":1573768739055,"user_tz":360,"elapsed":305,"user":{"displayName":"yuyo hakusho","photoUrl":"","userId":"15919232548116528887"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["# select a seed text\n","seed_text = lines[15]#lines[randint(0,len(lines))\n","sentences_to_indices(np.array([seed_text]),word_to_index, index_to_word, word_to_vec_map, max_len = length)"],"execution_count":134,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 62065., 357266., 316998., 359889., 360915., 129404., 154323.,\n","         43010.,  84884., 357212., 300216., 128527., 357266., 330916.,\n","        268046., 357266., 250836., 357266., 389245., 388590., 377946.,\n","        251645., 360915., 132701., 357266., 216334.,  87775., 239792.,\n","        177103.,  60665., 175199., 123517., 143869., 360915., 143869.,\n","        175199., 358464., 175199., 253767., 148427., 337259., 142467.,\n","        151349., 126981., 336114., 336114., 175199.,  71917.,  88126.,\n","        366274., 360915.]])"]},"metadata":{"tags":[]},"execution_count":134}]},{"cell_type":"code","metadata":{"id":"DPVOCNat_2V8","colab_type":"code","outputId":"790601c1-9bb4-4076-ddbd-363308ce7785","executionInfo":{"status":"ok","timestamp":1573764718685,"user_tz":360,"elapsed":348,"user":{"displayName":"yuyo hakusho","photoUrl":"","userId":"15919232548116528887"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(seed_text.split())"],"execution_count":97,"outputs":[{"output_type":"execute_result","data":{"text/plain":["51"]},"metadata":{"tags":[]},"execution_count":97}]},{"cell_type":"code","metadata":{"id":"6rRTCjCk4TqX","colab_type":"code","outputId":"abd0e734-7c1b-460a-d6ab-ecf822d92fd8","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1573769969506,"user_tz":360,"elapsed":2679,"user":{"displayName":"yuyo hakusho","photoUrl":"","userId":"15919232548116528887"}}},"source":["print(seed_text)\n","# generate 30 new words\n","generated = generate_seq(model, word_to_index, index_to_word, word_to_vec_map, seed_text,length,30,temperature = 0.5)\n","print(generated)"],"execution_count":139,"outputs":[{"output_type":"stream","text":["at the same time to drink from a brook that ran down the side of the mountain the wolf wished very much to eat the lamb but meeting her as he did face to face he thought he must find some excuse for doing so so he began by trying to\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n","  \"\"\"\n"],"name":"stderr"},{"output_type":"stream","text":["the was not you him the ran he meant find so of to him so he much snapped to trying to town ground cried small can go dog gently was\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T-HrfKYB6D87","colab_type":"code","outputId":"b6e5d5a4-aa81-46af-b1a4-9d3c4533e169","executionInfo":{"status":"ok","timestamp":1573770175396,"user_tz":360,"elapsed":1709,"user":{"displayName":"yuyo hakusho","photoUrl":"","userId":"15919232548116528887"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["seed_text = 'the wolf was licking his own butt and in the in really does not matter'\n","print(seed_text)\n","# generate 30 new words\n","generated = generate_seq(model, word_to_index, index_to_word, word_to_vec_map, seed_text,length,30,temperature=0)\n","print(generated)"],"execution_count":144,"outputs":[{"output_type":"stream","text":["the wolf was licking his own butt and in the in really does not matter\n","to his him his him him him him him him him him him him him what what what what what what what what what what what what what what what\n"],"name":"stdout"}]}]}